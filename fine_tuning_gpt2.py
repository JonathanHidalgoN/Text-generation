# -*- coding: utf-8 -*-
"""final_tune

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s0uPK_zgo-N9EC2AOI37L40_dbarglqx
"""


import torch
from torch.utils.data import Dataset, random_split
from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel


class Tweets(Dataset):
    """
    A dataset of tweets.
    Args:
        txt_list: A list of tweets.
        tokenizer: The tokenizer to use.
        max_length: The maximum length of a tweet.
    Inherits from: torch.utils.data.Dataset
    """

    def __init__(self, txt_list, tokenizer, max_length):
        self.input_ids = []
        self.attn_masks = []
        self.labels = []
        for txt in txt_list:
            encodings_dict = tokenizer(
                "<|startoftext|>" + txt + "<|endoftext|>",
                truncation=True,
                max_length=max_length,
                padding="max_length",
            )
            self.input_ids.append(torch.tensor(encodings_dict["input_ids"]))
            self.attn_masks.append(torch.tensor(encodings_dict["attention_mask"]))

    def __len__(self):
        """
        Custom method to get the length of the dataset.
        """
        return len(self.input_ids)

    def __getitem__(self, idx):
        """
        Custom method to get a sample from the dataset.
        """
        return self.input_ids[idx], self.attn_masks[idx]


def read_text(filename, delimiter=",", lines_to_return=1000):
    """
    Reads a text file and returns a list of lines.
    Args:
        filename: The name of the file to read.
        delimiter: The delimiter to use when reading the file.
        lines_to_return: The number of lines to return.
    Returns:
        A list of lines.
    """
    with open(filename, "r") as f:
        text = f.read()
    lines_to_return = text.split(delimiter)[:lines_to_return]
    return lines_to_return


run_status = True
if run_status:

    # This is runing in a kaggle notebook, you may need to change the path to the data or the parameters.
    # Define parameters
    token_parameters = {
        "model": "gpt2-medium",
        "bos_token": "<|startoftext|>",
        "eos_token": "<|endoftext|>",
        "pad_token": "<|pad|>",
    }
    data_parameters = {
        "path": "/kaggle/input/fran-tweets/fran_tweets.txt",
        "delimiter": "|-|",
        "to_read": 10000,
        "training_size": 0.9,
    }
    training_parameters = {
        "output_dir": "./results",
        "num_train_epochs": 1,
        "logging_steps": 100,
        "save_steps": 5000,
        "per_device_train_batch_size": 1,
        "per_device_eval_batch_size": 1,
        "warmup_steps": 10,
        "weight_decay": 0.05,
        "logging_dir": "./logs",
        "report_to": "none",
    }

    load_parameters = {
        "load_pretrained": True,
        "model": "/kaggle/input/fine-tune-models/fran_finetune_model.bin",
    }

    tokenizer = GPT2Tokenizer.from_pretrained(
        token_parameters["model"],
        bos_token=token_parameters["bos_token"],
        eos_token=token_parameters["eos_token"],
        pad_token=token_parameters["pad_token"],
    )

    if not load_parameters["load_pretrained"]:

        print("Training is starting soon ...")

        # Load model and tokenizer
        model = GPT2LMHeadModel.from_pretrained(token_parameters["model"]).cuda()
        model.resize_token_embeddings(len(tokenizer))

        # Load data
        tweets = read_text(
            filename=data_parameters["path"],
            delimiter=data_parameters["delimiter"],
            lines_to_return=data_parameters["to_read"],
        )
        max_length = max([len(tokenizer.encode(tweet)) for tweet in tweets])

        # Deine the data as Tweets class because torch needs it as a Dataset
        dataset = Tweets(tweets, tokenizer, max_length=max_length)
        # Split the data into training and validation
        train_size = int(data_parameters["training_size"] * len(dataset))
        train_dataset, val_dataset = random_split(
            dataset, [train_size, len(dataset) - train_size]
        )
        # Define the training arguments as a TrainingArguments class because torch needs it as a TrainingArguments
        training_args = TrainingArguments(
            output_dir=training_parameters["output_dir"],
            num_train_epochs=training_parameters["num_train_epochs"],
            logging_steps=training_parameters["logging_steps"],
            save_steps=training_parameters["save_steps"],
            per_device_train_batch_size=training_parameters[
                "per_device_train_batch_size"
            ],
            per_device_eval_batch_size=training_parameters[
                "per_device_eval_batch_size"
            ],
            warmup_steps=training_parameters["warmup_steps"],
            weight_decay=training_parameters["weight_decay"],
            logging_dir=training_parameters["logging_dir"],
            report_to=training_parameters["report_to"],
        )

        # Define the trainer and train the model
        Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=lambda data: {
                "input_ids": torch.stack([f[0] for f in data]),
                "attention_mask": torch.stack([f[1] for f in data]),
                "labels": torch.stack([f[0] for f in data]),
            },
        ).train()

    else:
        print("Loading pretrained model...")
        model = GPT2LMHeadModel.from_pretrained(token_parameters["model"]).cuda()
        model.resize_token_embeddings(len(tokenizer))
        model.load_state_dict(torch.load(load_parameters["model"]))
        model.eval()

# Generate tweets

generation_parameters = {
    "do_sample": True,
    "top_k": 50,
    "max_length": 300,
    "top_p": 0.95,
    "temperature": 1.9,
    "num_return_sequences": 1,
}

prompts = ["IA is going to", "keras is", "the mind", "Working on"]


def generate_text(prompt, **generation_parameters):
    generated = tokenizer(prompt, return_tensors="pt").input_ids.cuda()
    sample_outputs = model.generate(
        generated,
        do_sample=generation_parameters["do_sample"],
        top_k=generation_parameters["top_k"],
        max_length=generation_parameters["max_length"],
        top_p=generation_parameters["top_p"],
        temperature=generation_parameters["temperature"],
        num_return_sequences=generation_parameters["num_return_sequences"],
        pad_token_id=tokenizer.eos_token_id,
    )
    texts = []
    for output in sample_outputs:
        text = tokenizer.decode(output, skip_special_tokens=True)
        texts.append(text)

    return texts


texts = [generate_text(prompt, **generation_parameters) for prompt in prompts]
