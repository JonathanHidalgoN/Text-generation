# -*- coding: utf-8 -*-
"""textgen3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SoJ3-nBjx6k_Zntb3ETL45yntyzvAHDs
"""

import numpy as np
from re import sub
from random import shuffle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras import layers
from tensorflow.keras import utils as ku
from tensorflow.keras import Input, Model
from collections import Counter
from random import shuffle
from random import randint

def read_tweets(path):
  with open(path, "r") as f:
    text = f.read()
  return text.split("|-|")

def clean_tweet(tweet):
  new_tweet = sub("\n"," ",tweet)
  new_tweet = sub("'","",new_tweet)
  return new_tweet

def get_tweets(path):
  tweets = read_tweets(path)
  tweets = list(map(clean_tweet,tweets))
  shuffle(tweets)
  return tweets

def find_max_len_tweet(tweets):
  max_str = max(tweets, key = len)
  max_len = len(max_str)
  return max_len

def count_words(tweets, returned_vals = 20_000):
  freq = Counter((" ".join(tweets)).split(" ")).most_common(returned_vals)
  return freq

def create_token(corpus):
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(corpus)
  return tokenizer

def truncate_tweets(tweets, max_words = 280):
  truncator = lambda string : string[:max_words + 1]
  truncated_tweets = map(truncator, tweets)
  return truncated_tweets

def encode_tweets(tweets, tokenizer):
  input_sequences = []
  for tweet in tweets:
    encoded_tweet = tokenizer.texts_to_sequences([tweet])[0]
    for i in range(1, len(encoded_tweet)):
      n_gram_sequence = encoded_tweet[:i+1]
      input_sequences.append(n_gram_sequence)
  return input_sequences

def decode_tweet(encoded_tweet, tokenizer):
  words_dict = tokenizer.index_word
  full_tweet = encoded_tweet[-1]
  sentence = [words_dict[key] for key in full_tweet]
  return " ".join(sentence)

def generate_train_data(sequences, max_len, total_words):
  padded_sequence = np.array(pad_sequences(sequences, 
                                          maxlen=max_len, 
                                          padding='pre'))
  ready_data = padded_sequence[:,:-1]
  label = padded_sequence[:,-1]
  label = ku.to_categorical(label, num_classes=total_words)

  return ready_data, label

take_first_element = lambda x : x[0]

def create_model(num_tokens):
  
  inputs = Input(shape=(None,), dtype="int64")
  embedded = layers.Embedding(input_dim=num_tokens, output_dim=10)(inputs)
  x = layers.Bidirectional(layers.LSTM(32))(embedded)
  x = layers.Dropout(0.5)(x)
  outputs = layers.Dense(num_tokens, activation="softmax")(x)
  model = Model(inputs, outputs)
  model.compile(optimizer="adam",
  loss="categorical_crossentropy"
  )
  return model

path = "/content/cleaned_tweets.txt"
tweets = get_tweets(path)
#Important to shuffle
shuffle(tweets)
total_words = 12_000
commond_words_freq = count_words(tweets, total_words)
commond_words = list(map(take_first_element, commond_words_freq))
commond_words_freq[:5]

max_len = find_max_len_tweet(tweets)
tokenizer = create_token(commond_words)    
#Add one word for uknown words. myabe change this in future
total_words +=1
print(max_len)
tweets = list(truncate_tweets(tweets,280))
max_len = find_max_len_tweet(tweets)

#encoded_tweets = encode_tweets(tweets[0:10], tokenizer)
#encoded_tweets[:10]

#decoded_tweet = decode_tweet(encoded_tweets,tokenizer)
#decoded_tweet

#Consume mucha ram, transformar cuando se usa?
#ready_tweets, labels = generate_train_data(encoded_tweets, max_len, total_words)
#print(max_len,total_words)
#print(ready_tweets.shape, labels.shape)

#ready_tweets

#labels[0].shape

model = create_model(total_words)
model.summary()

def take_random_batch(num_tweets, batch_size = 1000):
  num_batches = num_tweets // batch_size
  r_n = randint(0,num_batches)
  lower_index = (r_n) * batch_size
  upper_index = (r_n + 1) * batch_size
  return lower_index, upper_index

def generate_text(seed_text, next_words, model, max_sequence_len):

    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predict_x=model.predict(token_list, verbose = 0) 
        predicted=np.argmax(predict_x,axis=1)

        output_word = ""
        for word,index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += " "+output_word
    return seed_text

def train_model(tweets,model, epochs, callbacks = None):
  num_tweets = len(tweets)
  for _ in range(epochs):
    lower_index, upper_index = take_random_batch(num_tweets)
    batch = tweets[lower_index:upper_index +1]
    encoded_tweets = encode_tweets(batch, tokenizer)
    ready_data, labels = generate_train_data(encoded_tweets, max_len, total_words)
    model.fit(ready_data, labels, verbose = 1)
    print(generate_text("hi how are you", 20, model, max_len))





