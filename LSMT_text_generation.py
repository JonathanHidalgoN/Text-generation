# -*- coding: utf-8 -*-
"""textgen3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SoJ3-nBjx6k_Zntb3ETL45yntyzvAHDs
"""

import numpy as np
from re import sub
from random import shuffle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras import layers
from tensorflow.keras import utils as ku
from tensorflow.keras import Input, Model
from collections import Counter
from random import shuffle
from random import randint

def read_tweets(path):
  """
  Reads tweets from a file
  Args:
    path: path to the file
  Returns:
    tweets: list of tweets
  """
  with open(path, "r") as f:
    text = f.read()
  return text.split("|-|")

def clean_tweet(tweet):
  """
  Cleans a tweet
  Args:
    tweet: string
  Returns:
    tweet: cleaned string
  """
  new_tweet = sub("\n"," ",tweet)
  new_tweet = sub("'","",new_tweet)
  return new_tweet

def get_tweets(path):
  """
  Get tweets from a file to clean them
  Args:
    path: path to the file
  Returns:
    tweets: list of tweets
  """
  tweets = read_tweets(path)
  tweets = list(map(clean_tweet,tweets))
  shuffle(tweets)
  return tweets

def find_max_len_tweet(tweets):
  """
  Finds the maximum length of a tweet
  Args:
    tweets: list of tweets
  Returns:
    max_len: maximum length of a tweet
  """
  max_str = max(tweets, key = len)
  max_len = len(max_str)
  return max_len

def count_words(tweets, returned_vals = 20_000):
  """
  Counts the words in a list of tweets
  Args:
    tweets: list of tweets
    returned_vals: number of words to return
  Returns:
    freq: dict of words and their frequency
  """
  freq = Counter((" ".join(tweets)).split(" ")).most_common(returned_vals)
  return freq

def create_token(corpus):
  """
  Creates a tokenizer
  Args:
    corpus: list of tweets
  Returns:
    tokenizer: tokenizer
  """
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(corpus)
  return tokenizer

def truncate_tweets(tweets, max_words = 280):
  """
  Truncates tweets to a maximum number of words
  Args:
    tweets: list of tweets
    max_words: maximum number of words
  Returns:
    tweets: list of tweets
  """
  truncator = lambda string : string[:max_words + 1]
  truncated_tweets = map(truncator, tweets)
  return truncated_tweets

def encode_tweets(tweets, tokenizer):
  """
  Encodes tweets
  Args:
    tweets: list of tweets
    tokenizer: tokenizer
  Returns:
    encoded_tweets: list of encoded tweets
  """
  input_sequences = []
  for tweet in tweets:
    encoded_tweet = tokenizer.texts_to_sequences([tweet])[0]
    for i in range(1, len(encoded_tweet)):
      n_gram_sequence = encoded_tweet[:i+1]
      input_sequences.append(n_gram_sequence)
  return input_sequences

def decode_tweet(encoded_tweet, tokenizer):
  """
  Decodes an encoded tweet
  Args:
    encoded_tweet: encoded tweet
    tokenizer: tokenizer
  Returns:
    decoded_tweet: decoded tweet
  """
  words_dict = tokenizer.index_word
  full_tweet = encoded_tweet[-1]
  sentence = [words_dict[key] for key in full_tweet]
  return " ".join(sentence)

def generate_train_data(sequences, max_len, total_words):
  """
  Generates training data
  Args:
    sequences: list of encoded tweets
    max_len: maximum length of a tweet
    total_words: number of words in the corpus
  Returns:
    ready_data: input data
    label: output data
  """
  padded_sequence = np.array(pad_sequences(sequences, 
                                          maxlen=max_len, 
                                          padding='pre'))
  ready_data = padded_sequence[:,:-1]
  label = padded_sequence[:,-1]
  label = ku.to_categorical(label, num_classes=total_words)

  return ready_data, label

take_first_element = lambda x : x[0]

def create_model(num_tokens):
  """
  Creates a model
  Args:
    num_tokens: number of tokens
  Returns:
    model: model
  """
  inputs = Input(shape=(None,), dtype="int64")
  embedded = layers.Embedding(input_dim=num_tokens, output_dim=10)(inputs)
  x = layers.Bidirectional(layers.LSTM(32))(embedded)
  x = layers.Dropout(0.5)(x)
  outputs = layers.Dense(num_tokens, activation="softmax")(x)
  model = Model(inputs, outputs)
  model.compile(optimizer="adam",
  loss="categorical_crossentropy"
  )
  return model

path = "/content/cleaned_tweets.txt"
tweets = get_tweets(path)
#Important to shuffle
shuffle(tweets)
total_words = 12_000
commond_words_freq = count_words(tweets, total_words)
commond_words = list(map(take_first_element, commond_words_freq))
commond_words_freq[:5]

max_len = find_max_len_tweet(tweets)
tokenizer = create_token(commond_words)    
#Add one word for uknown words. myabe change this in future
total_words +=1
print(max_len)
tweets = list(truncate_tweets(tweets,280))
max_len = find_max_len_tweet(tweets)

#encoded_tweets = encode_tweets(tweets[0:10], tokenizer)
#encoded_tweets[:10]

#decoded_tweet = decode_tweet(encoded_tweets,tokenizer)
#decoded_tweet

#Consume mucha ram, transformar cuando se usa?
#ready_tweets, labels = generate_train_data(encoded_tweets, max_len, total_words)
#print(max_len,total_words)
#print(ready_tweets.shape, labels.shape)

#ready_tweets

#labels[0].shape

model = create_model(total_words)
#model.summary()

def take_random_batch(num_tweets, batch_size = 1000):
  """
  Takes a random batch of tweets
  Args:
    num_tweets: number of tweets
    batch_size: size of the batch
  Returns:
  lower_index: lower index of the batch
  upper_index: upper index of the batch
  """
  num_batches = num_tweets // batch_size
  r_n = randint(0,num_batches)
  lower_index = (r_n) * batch_size
  upper_index = (r_n + 1) * batch_size
  return lower_index, upper_index

def generate_text(seed_text, next_words, model, max_sequence_len):
    """
    Generates text
    Args:
      seed_text: seed text
      next_words: number of words to generate
      model: model
      max_sequence_len: maximum length of a tweet
    Returns:
      output_text: generated text
    """
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predict_x=model.predict(token_list, verbose = 0) 
        predicted=np.argmax(predict_x,axis=1)

        output_word = ""
        for word,index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += " "+output_word
    return seed_text

def train_model(tweets,model, epochs, callbacks = None):
  """
  Trains a model
  Args:
    tweets: list of tweets
    model: model
    epochs: number of epochs
    callbacks: callbacks
  Returns:
    model: trained model
  """
  num_tweets = len(tweets)
  for _ in range(epochs):
    lower_index, upper_index = take_random_batch(num_tweets)
    batch = tweets[lower_index:upper_index +1]
    encoded_tweets = encode_tweets(batch, tokenizer)
    ready_data, labels = generate_train_data(encoded_tweets, max_len, total_words)
    model.fit(ready_data, labels, verbose = 1)
    print(generate_text("hi how are you", 20, model, max_len))





